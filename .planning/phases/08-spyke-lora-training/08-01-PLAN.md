---
phase: 08-spyke-lora-training
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml
autonomous: false
requirements:
  - LORA-04
must_haves:
  truths:
    - "train_network.py exists at ~/tools/kohya_ss/sd-scripts/train_network.py"
    - "networks/lora.py exists at ~/tools/kohya_ss/sd-scripts/networks/lora.py"
    - "spyke_lora_dataset.toml exists and references correct train and reg image dirs"
    - "MPS is confirmed as the accelerate device (Accelerator device: mps)"
    - "A 5-step smoke test completes without error and without NaN loss"
  artifacts:
    - path: ".planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml"
      provides: "Dataset config for kohya_ss train_network.py"
      contains: "spyke_plasma_v1"
    - path: "~/tools/kohya_ss/sd-scripts/train_network.py"
      provides: "The LoRA training script (populated by submodule init)"
  key_links:
    - from: "spyke_lora_dataset.toml"
      to: "/Users/dondemetrius/Code/plasma/dataset/spyke/train/10_spyke_plasma_v1"
      via: "image_dir field"
      pattern: "image_dir.*10_spyke_plasma_v1"
    - from: "spyke_lora_dataset.toml"
      to: "/Users/dondemetrius/Code/plasma/dataset/spyke/reg/1_anime_character"
      via: "is_reg = true subset image_dir field"
      pattern: "is_reg.*true"
---

<objective>
Fix the sd-scripts submodule (train_network.py is missing), write the dataset TOML config, verify MPS is active via Python device check, and run a 5-step smoke test to confirm the full training stack works before committing to a 70+ minute run.

Purpose: Eliminate the "blocker zero" discovered in research — sd-scripts is empty, so no training command can run — and satisfy LORA-04 (MPS confirmed active before full training).
Output: Populated sd-scripts directory, spyke_lora_dataset.toml, confirmed MPS device, passing 5-step smoke test.
</objective>

<execution_context>
@/Users/dondemetrius/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dondemetrius/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-spyke-lora-training/08-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix sd-scripts submodule and write dataset TOML</name>
  <files>.planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml</files>
  <action>
Run the submodule fix command to populate ~/tools/kohya_ss/sd-scripts/:

```bash
cd ~/tools/kohya_ss
git submodule update --init --recursive
```

Verify the two required files now exist:
```bash
ls ~/tools/kohya_ss/sd-scripts/train_network.py
ls ~/tools/kohya_ss/sd-scripts/networks/lora.py
```

Both must be present and non-empty. If either is missing, the submodule init failed — check git output for errors before continuing.

Then create the output directory for training artifacts:
```bash
mkdir -p /Users/dondemetrius/Code/plasma/output/loras/spyke/
```

Then write the dataset TOML config file at:
`.planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml`

Contents:
```toml
[general]
enable_bucket = true
caption_extension = ".txt"
shuffle_caption = true
keep_tokens = 1

[[datasets]]
resolution = 512
batch_size = 1

  [[datasets.subsets]]
  image_dir = "/Users/dondemetrius/Code/plasma/dataset/spyke/train/10_spyke_plasma_v1"
  num_repeats = 10

  [[datasets.subsets]]
  is_reg = true
  image_dir = "/Users/dondemetrius/Code/plasma/dataset/spyke/reg/1_anime_character"
  num_repeats = 1
```

Field notes:
- `keep_tokens = 1` — keeps `spyke_plasma_v1` trigger word at position 0 (not shuffled)
- `is_reg = true` — marks reg images so trigger word learning is not applied to them
- `num_repeats = 10` in TOML is equivalent to the folder-prefix approach; included for explicitness
- Do NOT add `flip_aug` anywhere — Spyke's asymmetric costume makes flip augmentation destructive
  </action>
  <verify>
```bash
ls ~/tools/kohya_ss/sd-scripts/train_network.py
ls ~/tools/kohya_ss/sd-scripts/networks/lora.py
cat /Users/dondemetrius/Code/plasma/.planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml
```
All three outputs must be non-empty.
  </verify>
  <done>train_network.py exists at ~/tools/kohya_ss/sd-scripts/train_network.py; networks/lora.py exists; spyke_lora_dataset.toml is written with correct image_dir paths and is_reg = true on the reg subset.</done>
</task>

<task type="auto">
  <name>Task 2: MPS device check</name>
  <files></files>
  <action>
Run the Python MPS verification script inside the kohya_ss venv:

```bash
source ~/tools/kohya_ss/venv/bin/activate
export PYTORCH_ENABLE_MPS_FALLBACK=1
python3 -c "
import torch
from accelerate import Accelerator
print('PyTorch version:', torch.__version__)
print('MPS available:', torch.backends.mps.is_available())
print('MPS built:', torch.backends.mps.is_built())
a = Accelerator()
print('Accelerator device:', a.device)
print('Mixed precision:', a.mixed_precision)
"
```

Expected output:
```
PyTorch version: 2.5.1
MPS available: True
MPS built: True
Accelerator device: mps
Mixed precision: no
```

If `Accelerator device:` shows `cpu` instead of `mps`, the accelerate config has changed. Fix with:
```bash
accelerate config
```
Then answer the prompts: "This machine", "No distributed training", "no" to TPU, "no" to fp16/bf16 (select "no" for mixed precision), and confirm it saves.

If `MPS available: False`, stop and report — MPS should be available on this M1 Pro machine per Phase 5 verification.
  </action>
  <verify>
Python output shows `Accelerator device: mps` and `Mixed precision: no`. No exceptions thrown.
  </verify>
  <done>Accelerator device is mps, mixed precision is no, MPS available is True — all three confirmed in printed output.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Run 5-step smoke test and verify MPS active (LORA-04)</name>
  <files></files>
  <action>
This is a human-run verification step. Run the 5-step smoke test command below. This confirms the full command syntax, dataset loading, and MPS routing work before launching the real 920-step run. Expected runtime: 30-90 seconds.

IMPORTANT: Close ComfyUI and other GPU-intensive apps first.

```bash
export PYTORCH_ENABLE_MPS_FALLBACK=1
source ~/tools/kohya_ss/venv/bin/activate

accelerate launch \
  --num_cpu_threads_per_process=4 \
  ~/tools/kohya_ss/sd-scripts/train_network.py \
  --pretrained_model_name_or_path="$HOME/tools/ComfyUI/models/checkpoints/AnythingXL_inkBase.safetensors" \
  --dataset_config="/Users/dondemetrius/Code/plasma/.planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml" \
  --output_dir="/tmp/spyke_smoke_test/" \
  --output_name="smoke_test" \
  --save_model_as="safetensors" \
  --max_train_steps=5 \
  --learning_rate=1e-4 \
  --network_module="networks.lora" \
  --network_dim=32 \
  --network_alpha=16 \
  --mixed_precision="no" \
  --save_precision="float" \
  --optimizer_type="AdamW" \
  --no_half_vae \
  --gradient_checkpointing \
  --seed=42
```

What to check during and after the run:
1. First lines should show MPS device — look for `mps` or `mps:0` in accelerate state output
2. No `ValueError: fp16 mixed precision requires a GPU` error
3. No `FileNotFoundError` for train_network.py
4. `loss:` appears in stdout after each step — should be a finite number, NOT `nan`
5. Open Activity Monitor (Window menu -> GPU History) while the run is happening — GPU History panel should show non-zero utilization (this satisfies LORA-04)
6. After completion: `ls /tmp/spyke_smoke_test/` should show at least one file

If loss is `nan` from step 1: `--no_half_vae` flag was likely not accepted. Check command syntax.
If `FileNotFoundError` for train_network.py: submodule fix failed — re-run `git submodule update --init --recursive`.

Resume signal: Type "approved" if smoke test completed with finite loss and GPU History showed utilization. Describe any errors or unexpected output.
  </action>
  <verify>
Smoke test completed without errors. `loss:` values are finite (not nan). Activity Monitor GPU History showed non-zero utilization during the run (LORA-04 satisfied).
  </verify>
  <done>5-step smoke test ran to completion with finite loss values and MPS confirmed active via Activity Monitor GPU History — environment is validated for the full training run.</done>
</task>

</tasks>

<verification>
1. `ls ~/tools/kohya_ss/sd-scripts/train_network.py` — exits 0
2. `ls ~/tools/kohya_ss/sd-scripts/networks/lora.py` — exits 0
3. `cat .planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml` — shows both image_dir entries
4. Python MPS check prints `Accelerator device: mps`
5. Smoke test completed with finite `loss:` values and no errors
6. Activity Monitor GPU History showed non-zero utilization during smoke test (LORA-04)
</verification>

<success_criteria>
All three of: train_network.py exists, dataset TOML is written with correct paths, and a 5-step smoke test completes without error with MPS active — confirming the environment is ready for the full training run.
</success_criteria>

<output>
After completion, create `.planning/phases/08-spyke-lora-training/08-spyke-lora-training-01-SUMMARY.md`
</output>
