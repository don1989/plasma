---
phase: 08-spyke-lora-training
plan: "02"
type: execute
wave: 2
depends_on:
  - "08-01"
files_modified: []
autonomous: false
requirements:
  - LORA-01
  - LORA-02
must_haves:
  truths:
    - "Training runs to completion (920 steps across 4 epochs)"
    - "At least 4 checkpoint files exist in output/loras/spyke/ (steps 200, 400, 600, 800)"
    - "Final checkpoint spyke_plasma_v1.safetensors exists in output/loras/spyke/"
    - "loss: values were finite throughout training (no NaN)"
    - "MPS was active during training (GPU History showed utilization)"
  artifacts:
    - path: "output/loras/spyke/spyke_plasma_v1-step00000200.safetensors"
      provides: "Step 200 checkpoint"
    - path: "output/loras/spyke/spyke_plasma_v1-step00000400.safetensors"
      provides: "Step 400 checkpoint"
    - path: "output/loras/spyke/spyke_plasma_v1-step00000600.safetensors"
      provides: "Step 600 checkpoint"
    - path: "output/loras/spyke/spyke_plasma_v1-step00000800.safetensors"
      provides: "Step 800 checkpoint"
    - path: "output/loras/spyke/spyke_plasma_v1.safetensors"
      provides: "Final checkpoint (step 920, no step suffix)"
  key_links:
    - from: "accelerate launch command"
      to: "output/loras/spyke/"
      via: "--output_dir flag"
      pattern: "output_dir.*output/loras/spyke"
    - from: "train_network.py"
      to: "output/loras/spyke/spyke_plasma_v1-step*.safetensors"
      via: "--save_every_n_steps=200"
      pattern: "spyke_plasma_v1-step[0-9]+"
---

<objective>
Run the full 4-epoch (920-step) LoRA training run for Spyke using kohya_ss train_network.py on Apple Silicon with MPS acceleration. Training produces 4 intermediate checkpoints (every 200 steps) plus a final checkpoint — all 5 safetensors files are inputs to Plan 08-03 checkpoint selection.

Purpose: Produce the trained LoRA weight files that encode Spyke's visual identity into the SD 1.5 base model — the core deliverable of v2.0 character consistency.
Output: 5 safetensors files in output/loras/spyke/ (steps 200, 400, 600, 800, and 920 final).
</objective>

<execution_context>
@/Users/dondemetrius/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dondemetrius/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-spyke-lora-training/08-RESEARCH.md
@.planning/phases/08-spyke-lora-training/08-spyke-lora-training-01-SUMMARY.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 1: Launch full 920-step training run</name>
  <files></files>
  <action>
This is a human-run action. The training takes 15-60 minutes and produces all checkpoint files.

Before running: Close ComfyUI, browsers with GPU-intensive tabs (WebGL, video), and any other GPU-intensive applications. Training and ComfyUI running simultaneously causes GPU memory contention on 16GB unified memory.

Open Activity Monitor (Window menu -> GPU History) so you can observe GPU utilization during training.

Run the full training command:

```bash
export PYTORCH_ENABLE_MPS_FALLBACK=1
source ~/tools/kohya_ss/venv/bin/activate

accelerate launch \
  --num_cpu_threads_per_process=4 \
  ~/tools/kohya_ss/sd-scripts/train_network.py \
  --pretrained_model_name_or_path="$HOME/tools/ComfyUI/models/checkpoints/AnythingXL_inkBase.safetensors" \
  --dataset_config="/Users/dondemetrius/Code/plasma/.planning/phases/08-spyke-lora-training/spyke_lora_dataset.toml" \
  --output_dir="/Users/dondemetrius/Code/plasma/output/loras/spyke/" \
  --output_name="spyke_plasma_v1" \
  --save_model_as="safetensors" \
  --save_every_n_steps=200 \
  --max_train_epochs=4 \
  --learning_rate=1e-4 \
  --unet_lr=1e-4 \
  --text_encoder_lr=5e-5 \
  --lr_scheduler="cosine" \
  --lr_warmup_steps=0 \
  --network_module="networks.lora" \
  --network_dim=32 \
  --network_alpha=16 \
  --mixed_precision="no" \
  --save_precision="float" \
  --optimizer_type="AdamW" \
  --no_half_vae \
  --clip_skip=2 \
  --prior_loss_weight=1.0 \
  --max_data_loader_n_workers=1 \
  --persistent_data_loader_workers \
  --gradient_checkpointing \
  --seed=42
```

Step math:
- 23 images x 10 repeats / 1 batch = 230 steps per epoch
- 4 epochs = 920 total steps
- Checkpoint saves at: step 200, 400, 600, 800 (plus final at step 920)

Expected checkpoint files when training completes:
```
output/loras/spyke/spyke_plasma_v1-step00000200.safetensors
output/loras/spyke/spyke_plasma_v1-step00000400.safetensors
output/loras/spyke/spyke_plasma_v1-step00000600.safetensors
output/loras/spyke/spyke_plasma_v1-step00000800.safetensors
output/loras/spyke/spyke_plasma_v1.safetensors  (final, no step suffix)
```

What to monitor during training:
- `loss:` values should be finite (not `nan`) — if loss is NaN from step 1, add `--cache_latents` and retry
- GPU History should show sustained non-zero utilization
- Expected time: 15-60 minutes on M1 Pro (confirmed by smoke test step timing)

If training crashes with memory error partway through, add `--cache_latents` to the command and restart.
If loss becomes NaN mid-training (not from step 1), the run can still be salvaged — checkpoints saved before the NaN step are valid.

Resume signal: Type "training complete" and paste the final lines of the training log (showing the last loss: value and step count). Also run `ls -lh /Users/dondemetrius/Code/plasma/output/loras/spyke/` and paste the output.
  </action>
  <verify>
```bash
ls -lh /Users/dondemetrius/Code/plasma/output/loras/spyke/
```
Expected: 5 files (4 step checkpoints + 1 final), each 50-200MB, no zero-byte files.

Training log should show loss values at step 920 that are finite (not NaN or inf).
  </verify>
  <done>All 5 checkpoint files exist in output/loras/spyke/ with sizes 50-200MB each, and training log shows finite loss at the final step.</done>
</task>

</tasks>

<verification>
```bash
ls -lh /Users/dondemetrius/Code/plasma/output/loras/spyke/
```
Expected: 5 files (4 step checkpoints + 1 final), each 50-200MB, no zero-byte files.

Training log shows loss values at step 920 are finite (not NaN or inf).
</verification>

<success_criteria>
All 5 checkpoint files exist in output/loras/spyke/: spyke_plasma_v1-step00000200.safetensors, spyke_plasma_v1-step00000400.safetensors, spyke_plasma_v1-step00000600.safetensors, spyke_plasma_v1-step00000800.safetensors, and spyke_plasma_v1.safetensors — with finite loss values throughout the run.
</success_criteria>

<output>
After completion, create `.planning/phases/08-spyke-lora-training/08-spyke-lora-training-02-SUMMARY.md`
</output>
